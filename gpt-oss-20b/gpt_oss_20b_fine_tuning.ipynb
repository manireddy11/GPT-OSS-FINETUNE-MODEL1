{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_section"
         
      },
      "source": [
        "# Fine-tuning GPT-OSS (20B) as a Multilingual Reasoning Model with Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "why_section"
      },
      "source": [
        "In this notebook, we'll walkthrough how OpenAI's open-weight reasoning model `gpt-oss-20b` can be fine-tuned to **reason effectively in multiple languages**. We'll do this by:\n",
        "1. Fine-tuning with [Unsloth](https://github.com/unslothai/unsloth) for 2x faster training\n",
        "2. Using a multilingual reasoning dataset where chain-of-thought has been translated\n",
        "\n",
        "The end result? A model that can generate reasoning steps in French, Spanish, German, Italian, and other languages. You can even **mix languages**:\n",
        "\n",
        "```\n",
        "User: I buy a book for $15 and a coffee for $4.50. The sales tax is 8%. What is the total cost?\n",
        "System: reasoning language: French\n",
        "\n",
        "Assistant reasoning (in French):\n",
        "L'utilisateur achÃ¨te un livre pour 15$ et un cafÃ© pour 4,50$.\n",
        "Le total avant taxes est 15 + 4,50 = 19,50$.\n",
        "La taxe de vente est de 8%, donc 19,50 Ã— 0,08 = 1,56$.\n",
        "Le coÃ»t total est 19,50 + 1,56 = 21,06$.\n",
        "\n",
        "Assistant response:\n",
        "The total cost is $21.06.\n",
        "```\n",
        "\n",
        "This enables AI developers working with under-represented languages to improve the interpretability of reasoning models in their native languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_section"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# We're installing the latest Torch, Triton, OpenAI's Triton kernels, Transformers and Unsloth!\n",
        "!pip install --upgrade -qqq uv\n",
        "try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "except: get_numpy = \"numpy\"\n",
        "!uv pip install -qqq \\\n",
        "    \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \\\n",
        "    \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
        "    \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
        "    git+https://github.com/huggingface/transformers \\\n",
        "    git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loading_section"
      },
      "source": [
        "### Loading the Model with Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560,
          "referenced_widgets": [
            "6492ab318f9e410ca3c1fa4290a507ab",
            "a14f446285d8463b8aecb0d8eb28b578",
            "6033dd9d87b64cb195a0fcaa360408d0",
            "f83b53d35f7e42aca45aa2faed1c5f1d",
            "ccfed9e18b804db693887ceb3d49dc28",
            "125b872f3c4340bebbc832fc6f35e9f4",
            "37a82cd7c8474e52b630d2c214bf6326",
            "1535d02dfa92440f9b2148e04bf1e339",
            "51479f26e5e14246a1b4de773b693ab7",
            "4df2f91ba1514c57a1cf5640760c85fa",
            "7d2c975b16e84b3c868ec551db2441da",
            "621874bafbb145e9b04ec30a900238d8",
            "c513dba33e65402fbf3b59d8737a3b4e",
            "6d869e9ab1c04db9a25af5cf1cf0d379",
            "b3e6cb151caf46568e78bc7c68c0d223",
            "14214940abe74c53be92848350bcbb67",
            "eb2175e0bca24b94a4f5a5dfb74436a0",
            "a731859e95de470db56179afe03c47c7",
            "cef3c871590045fe984ce8e6da988f31",
            "713719c1da1945c194b6b799f36d859f",
            "8bfb0277373348739e85d2b919eaeac3",
            "ed3b26b45c4a43a3a3978f7f7c433152",
            "2b6d392320ea47b2ba64c21f0368df9c",
            "72c620b4509b485c8b9a3990ca4cf872",
            "3953712de72545f89e27ffb85a843004",
            "d407cd063fc74397a2e135778417d606",
            "dfa0259cf1f04a90aa8117e3027d62f9",
            "faae10584cf343528cc0937b4cfa4a0c",
            "39ba69319b0249c5a638cf078979a6b2",
            "ed7f2db1a293471d912b647bfb14238a",
            "88e458ba3a49495299cf973ed2be8379",
            "d977a54c34d5467da62ffed88f2b8fb3",
            "f35214d1c859439fb35df6be691db577",
            "d6cd588a77524e96ac5d88e7cdb255da",
            "71874ecabdc64386908071887f3acfb3",
            "7e463d6458ac4034bacb9e87ed01c5d3",
            "f608996a603e413fbfe7effbe4a47147",
            "62bb5f8e74614dcca47fd2d6c3c741f6",
            "f490727ba8384fda87297bea8a407e05",
            "bdf7f020d28b4fc0a0b3cd483c88c4c7",
            "971ab27ae07f4e65a7adf2de73eaac4d",
            "5d8ae893357f4378b95d0ab36410f5be",
            "3626c60ba5954a8fbdce9b44226ba7bc",
            "ba52f8a6bb814fcf94676d4238d9fb02",
            "f92816a4d8bc40ebbdbc19c9d36691ca",
            "22b843230b44429cbaf17cb0792cf05c",
            "ef84221e6d9342bfad134ad1ca409b79",
            "cfb38cbf5f834144b0c50d5369425fcd",
            "b6547872d0844a8488320f815439868f",
            "8aa117e5451c4f158ae151fcccb20c23",
            "39f4d8b6404d4ae78816e9d05fe0096d",
            "8c81737a16e14d16af1c2a78ee43bd23",
            "210731199926400380c878190e29d702",
            "e7779365bba84625848ae0e9ab5f3d84",
            "4f0cf840ad6f43ac9d286208d90ee25f",
            "58c12e51b6964c48b32f58e6ec0ee11d",
            "26551edc00054cc18e30a91c72ef0279",
            "4759b46d9e234d088688e9edad98fd53",
            "e6e7c2de97a544f09879fb435c21ebc0",
            "7061412b6ad94671b27811ffb8f9f56c",
            "25fd5e1a35c64448a26710093b9b2412",
            "8edb7b2da08b416782402aa6ac142b99",
            "4d740298f4f3435f9ec147dd2d2e90f8",
            "153931a9f7a14a288a7b3327309acb36",
            "5fb231c22da04ab389bf28072c24a356",
            "7ed35b7087ad4be683767a68b6163194",
            "4ec592587a1646f0a566b633e60aacfe",
            "59f7b6c6630c4a59a7224181a88bb2d2",
            "019f8368aceb4525b83645e7f6b9a6db",
            "3d6c665e7d5e47ceaf63cc9f2f82e6ef",
            "4412b7ba65254691af67fa84d817cd7a",
            "129fdebd248d4a60982911c2ed032bff",
            "3162168ac12c4f04a314f1176bedad7d",
            "5f2daee48c1a407e8864e59521aff2c0",
            "9968d180a2ff43a6b7cc21a76e869e3b",
            "ac0dd4756519485b8e594ef5d0a466de",
            "5026e7b3aea94d26aa65be57f253a547",
            "aca0a7acb0d849ffa7ac5d6753430ebf",
            "31555d899ef94f1aa7996e5cba77ab66",
            "f4cedcff19e1412fbca6c0ee9be86c91",
            "d1d4239702d641848f383316e4cd277c",
            "16d82dca8fcf45048391c2870ba31ae9",
            "a5bb8975f05d4fc6a68dee41988353a9",
            "3b3df2d41f0a475f93431bb9ae93282f",
            "a8489d2b3f6d4a2492d3393a585ebeb3",
            "0e2c80d7614e4fbc929869e91561dd75",
            "5e8b5d7221d04de08b61a9af57cbebac",
            "0fe65d788c8d4aa2a533b20241128dd6",
            "937fbcee286d40cabc2ce0a1413c8f58",
            "45219720c26c4fa2b1f28585adbeda42",
            "b59ea1b8fa614901bcd97309f23882af",
            "3a9c3ecc79e647a99da41b303556f0fe",
            "ba2e0a11336b4684a52d95e1fcf0840c",
            "7c5808fb648f4b5695c6be21709052a4",
            "1227497fc5954734a15af0f62a1bb6bf",
            "c8e2dea5c86a48f999638c2b5d13d457",
            "3596f6dfd4b742bca36ee84c2f1f44dd",
            "e376cdf307cf43ef9d587155b46288eb",
            "e01ac1f128e747d5b13bd4602da845de",
            "7e6c60911c2a49f79bfa33669a70982a",
            "6788dffbce6d4d2f82b877433d97e402",
            "5e732267d6d9480c932b3cc4ff43ebc7",
            "b7157bd49c194f74a384ee2b060220bf",
            "09fc013e5f9d4a8ea5743c7ee12c9294",
            "6c0023314fc74423bab4d0df0c8636d2",
            "dff66e12e0ce4b04b7d0b95cfc09d2e9",
            "9802896f5f2a47d594870e1542a9a7e7",
            "a57e8b6bc3334c78ba0c7619ca064574",
            "3ca33586ae2c41d5a7780bafc9396d4e",
            "7cd9e35a912d436b8c265194742241c5",
            "97b71c87c2b7460aa8fbe2f360bc3d8e",
            "d42963de03b74b1384f3615f493672c3",
            "0104f59f509844cf90992252ea907346",
            "39538aa9376d4fd2886cbe37aa7e72d7",
            "575d459760be496685838eb7100a37d7",
            "6d863db1e0fb4bc4bead3d9f320e418a",
            "140cacda5df94f19887ff9bf2765ede3",
            "27e5bffb16734847954150e1ba1928b3",
            "e034c654a25749e18f4c36a35cd3534a",
            "d6902ef3e82a4192a11278b40f39df55",
            "f2e42c773858436fbd785321a30bc196"
          ]
    ]
        },
        "id": "load_model",
        "outputId": "69baa8d8-af9d-4a6e-f73a-7227fa48e0e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.8.9: Fast Gpt_Oss patching. Transformers: 4.56.0.dev0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gpt_oss won't work! Using float32.\n",
            "Unsloth: Gpt_Oss does not support SDPA - switching to fast eager.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6492ab318f9e410ca3c1fa4290a507ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "621874bafbb145e9b04ec30a900238d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b6d392320ea47b2ba64c21f0368df9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/3.37G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6cd588a77524e96ac5d88e7cdb255da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.16G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f92816a4d8bc40ebbdbc19c9d36691ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58c12e51b6964c48b32f58e6ec0ee11d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/165 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ec592587a1646f0a566b633e60aacfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aca0a7acb0d849ffa7ac5d6753430ebf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "937fbcee286d40cabc2ce0a1413c8f58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e6c60911c2a49f79bfa33669a70982a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97b71c87c2b7460aa8fbe2f360bc3d8e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
        "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
        "    \"unsloth/gpt-oss-120b\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gpt-oss-20b\",\n",
        "    dtype = dtype, # None for auto detection\n",
        "    max_seq_length = max_seq_length, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lora_section"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "add_lora",
        "outputId": "6afa4a05-b1a1-488f-eb8f-c7be3c2274af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reasoning_effort_section"
      },
      "source": [
        "### Understanding Reasoning Effort Levels\n",
        "\n",
        "The `gpt-oss` models include a unique feature that allows you to adjust the model's **\"reasoning effort\"**â€”controlling the trade-off between performance and response speed (latency) by adjusting how many tokens the model uses to think.\n",
        "\n",
        "----\n",
        "\n",
        "Three distinct levels:\n",
        "* **Low**: Optimized for fast responses, minimal multi-step reasoning\n",
        "* **Medium**: Balanced performance and speed\n",
        "* **High**: Strongest reasoning performance for complex tasks (higher latency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test_reasoning_low",
        "outputId": "63a3df08-3400-40ba-93a7-707bb9cd5ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
            "Knowledge cutoff: 2024-06\n",
            "Current date: 2025-08-26\n",
            "\n",
            "Reasoning: low\n",
            "\n",
            "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
            "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>user<|message|>I buy a book for $15 and a coffee for $4.50. The sales tax is 8%. What is the total cost?<|end|><|start|>assistant<|channel|>analysis<|message|>We compute: 15 + 4.5 = 19.5. Tax 8% of 19.5 = 1.56. Total = 21.06. We'll output.<|end|><|start|>assistant<|channel|>final<|message|>Stepâ€‘byâ€‘step:\n",
            "\n",
            "1. **Subtotal (before tax)**  \n",
            "   \\[\n",
            "   15.00\\ \\text{(book)} + 4.50\\ \\text{(coffee)} = 19.50\n",
            "   \\]\n",
            "\n",
            "2. **Sales tax**  \n",
            "   \\[\n",
            "   \\text{Tax} = 19.50 \\times 0.08 = 1.56\n",
            "   \\]\n",
            "\n",
            "3. **Total cost**  \n",
            "   \\[\n",
            "   \\text{Total} = 19.50 + 1.56 = 21.06\n",
            "   \\]\n",
            "\n",
            "**Total cost: \\$21.06**.<|return|>\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"I buy a book for $15 and a coffee for $4.50. The sales tax is 8%. What is the total cost?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"low\",\n",
        ").to(model.device)\n",
        "\n",
        "_ = model.generate(**inputs, max_new_tokens = 512, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_medium_text"
      },
      "source": [
        "Changing the `reasoning_effort` to `high` will make the model think longer. We have to increase the `max_new_tokens` to occupy the amount of the generated tokens but it will give better and more correct answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test_reasoning_medium",
        "outputId": "c68c6b81-1990-45ea-8abc-088a9fffa8d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
            "Knowledge cutoff: 2024-06\n",
            "Current date: 2025-08-26\n",
            "\n",
            "Reasoning: high\n",
            "\n",
            "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
            "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>user<|message|>I buy a book for $15 and a coffee for $4.50. The sales tax is 8%. What is the total cost?<|end|><|start|>assistant<|channel|>analysis<|message|>The user: \"I buy a book for $15 and a coffee for $4.50. The sales tax is 8%. What is the total cost?\" They want the total cost of the purchase. This is a straightforward arithmetic problem.\n",
            "\n",
            "Book cost: $15. Coffee cost: $4.50. Total before tax: $15 + $4.50 = $19.50.\n",
            "\n",
            "Sales tax 8% of the subtotal: 0.08 Ã— 19.50 = $1.56. (But we can compute exactly: 19.50 * 0.08 = (1950 * 0.08)/100 so 155.999? Wait: 19.50 * 8% = 19.50 * 0.08 = 1.56. Yes exactly since 19.5 * 8% = 1.56.\n",
            "\n",
            "Add tax to subtotal: $19.50 + $1.56 = $21.06.\n",
            "\n",
            "Thus total cost: $21.06.\n",
            "\n",
            "We will answer concisely. Provide explanation? user just wants the number probably. Provide the calculation.<|end|><|start|>assistant<|channel|>final<|message|>The subtotal before tax is\n",
            "\n",
            "\\[\n",
            "15 + 4.50 = \\$19.50.\n",
            "\\]\n",
            "\n",
            "The sales tax is 8â€¯%, so the tax amount is\n",
            "\n",
            "\\[\n",
            "0.08 \\times 19.50 = \\$1.56.\n",
            "\\]\n",
            "\n",
            "Adding the tax gives the total cost:\n",
            "\n",
            "\\[\n",
            "19.50 + 1.56 = \\$21.06.\n",
            "\\]\n",
            "\n",
            "So your total cost is **\\$21.06**.<|return|>\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"I buy a book for $15 and a coffee for $4.50. The sales tax is 8%. What is the total cost?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"high\",\n",
        ").to(model.device)\n",
        "\n",
        "_ = model.generate(**inputs, max_new_tokens = 2048, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_prep_section"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep: The Multilingual Reasoning Dataset\n",
        "\n",
        "We'll use the [HuggingFaceH4/Multilingual-Thinking](https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking) dataset.\n",
        "- It contains reasoning chains translated into multiple languages (French, Spanish, German, Italian)\n",
        "- Each example has both the reasoning process (`analysis` channel) and final answer (`final` channel)\n",
        "- By training on this, the model learns to generate reasoning steps in different languages\n",
        "\n",
        "### Understanding the Harmony Format\n",
        "\n",
        "The GPT-OSS models use OpenAI's Harmony format for conversations. Here's what each field means:\n",
        "\n",
        "| Field | Purpose |\n",
        "|-------|---------|\n",
        "| `developer` | Custom instructions for the model (similar to system role) |\n",
        "| `user` | User's input question |\n",
        "| `assistant` | Model's output with two special channels |\n",
        "| `analysis` | The model's chain-of-thought reasoning |\n",
        "| `final` | The final response shown to the user |\n",
        "\n",
        "The key innovation: The assistant response contains **two channels**:\n",
        "- **`analysis` channel**: Where the model thinks step-by-step (can be in any language)\n",
        "- **`final` channel**: The polished response to the user\n",
        "\n",
        "This separation allows the model to reason in one language while responding in another!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182,
          "referenced_widgets": [
            "25461a1a47724d0796dd76c51a930ed3",
            "c53032b8d4a44dbc8b6772c0e6f8fd4f",
            "9632a29f0cf24f77aaf40de103a93e87",
            "e952003e6b49494db8306772b76c4747",
            "5778ba2c10cd4a0e8b2c2637968ef9cc",
            "df35d4d360c5483684b3b53311f2a26e",
            "ecf2583fff9e4663afc9bef03ee19b2b",
            "7dc75c517afe42bfa7955c835b15f21c",
            "e8a60259027b4fa89293187ebbd39c32",
            "0a317bbf2d564db997142dcefbbf4cc3",
            "9407e8632a3f4c23b32506dbfc8f5633",
            "061083e658e9468890da97397c453f03",
            "3ffbdfe79e7a423b8853de995a3b8169",
            "4d2cdb152a964da3b589168459ba2fff",
            "54aeb314448c43f29b53fd26d96b291a",
            "f766d5e7c02342c5a5fd98a4de014aa5",
            "191f8eb7219646788baf2e7ebf9217af",
            "ffa5196280474e759fc3052845ef2ec7",
            "1c811e5e11274c21a0ccbcdfc73e16e0",
            "71cb06e127db4e2b8caa3e9719864eb3",
            "d6a12b954be0496389b02726b10cf195",
            "bf828e837e614811a0937272f8b18bb2",
            "5e24917997d54e86b687fc0bd599ca20",
            "ff016419a447499aa01dad158bf28c06",
            "f6c751bdb77b4bd195a2a02ca739a6c6",
            "3abe73517be242d8a513233ce446ef1f",
            "89a238cd871149539f7186ecac55f237",
            "461675a94d474e04849f50f92da0a17e",
            "e322d3e4357140c1bfaf8db4fbb41574",
            "ca7c6a48cd684d33a07f543008f61ec0",
            "0440a30c1d9149c8aedbf6befa6f6e56",
            "429afd2a393c40208dd7eea853583630",
            "8d8ee73185e740ea88ffc876c72ca4c0"
          ]
        },

